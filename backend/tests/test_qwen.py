"""
Qwen3 8B LLM í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import asyncio
import httpx
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.services.llm_service import LLMService
from app.api.v1.schemas.shelter import ShelterInfo
import uuid


async def test_qwen_direct():
    """Ollama API ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("Qwen3 8B ì§ì ‘ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì •ë¶€ì˜ ì¬ë‚œì•ˆì „ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í˜¸ìš° ê²½ë³´ ìƒí™©ì—ì„œ ì„œìš¸ì‹œ ì˜ë“±í¬êµ¬ì— ìˆëŠ” ì„±ì¸ì—ê²Œ ì¦‰ì‹œ í–‰ë™ ì§€ì¹¨ì„ ì •í™•íˆ 3~5ì¤„ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ê°€ì¥ ê°€ê¹Œìš´ ëŒ€í”¼ì†Œ:
1. ì˜ë“±í¬ì´ˆë“±í•™êµ (ë„ë³´ 3ë¶„)
2. ì˜ë“±í¬êµ¬ë¯¼ì²´ìœ¡ì„¼í„° (ë„ë³´ 5ë¶„)
3. ì˜ë“±í¬ë„ì„œê´€ (ë„ë³´ 7ë¶„)

ê·œì¹™: êµ¬ì²´ì  í–‰ë™ë§Œ, ê¸ˆì§€ì‚¬í•­ í¬í•¨, ì¶”ì¸¡ ì ˆëŒ€ ê¸ˆì§€

í–‰ë™ ì¹´ë“œ:"""
    
    try:
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "qwen3:8b-instruct",
                    "prompt": prompt,
                    "temperature": 0.3,
                    "stream": False
                }
            )
        
        if response.status_code == 200:
            result = response.json()
            print("\nâœ… Qwen3 ì‘ë‹µ:")
            print("-" * 60)
            print(result['response'])
            print("-" * 60)
        else:
            print(f"\nâŒ API ì˜¤ë¥˜: {response.status_code}")
            print(response.text)
    
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬: {str(e)}")
        print("\nğŸ’¡ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”:")
        print("   docker ps | grep ollama")
        print("   docker exec pes-ollama ollama list")


async def test_llm_service():
    """LLMService í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("LLMService í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    llm_service = LLMService()
    
    # ìƒ˜í”Œ ëŒ€í”¼ì†Œ ë°ì´í„°
    shelters = [
        ShelterInfo(
            id=uuid.uuid4(),
            name="ì˜ë“±í¬ì´ˆë“±í•™êµ",
            address="ì„œìš¸ì‹œ ì˜ë“±í¬êµ¬ ì˜ë“±í¬ë¡œ 123",
            shelter_type="ì´ˆë“±í•™êµ",
            capacity=500,
            distance_km=0.25,
            walking_minutes=3,
            latitude=37.5263,
            longitude=126.8962
        ),
        ShelterInfo(
            id=uuid.uuid4(),
            name="ì˜ë“±í¬êµ¬ë¯¼ì²´ìœ¡ì„¼í„°",
            address="ì„œìš¸ì‹œ ì˜ë“±í¬êµ¬ ì˜ë“±í¬ë¡œ 234",
            shelter_type="ì²´ìœ¡ê´€",
            capacity=800,
            distance_km=0.40,
            walking_minutes=5,
            latitude=37.5280,
            longitude=126.9000
        ),
        ShelterInfo(
            id=uuid.uuid4(),
            name="ì˜ë“±í¬ë„ì„œê´€",
            address="ì„œìš¸ì‹œ ì˜ë“±í¬êµ¬ ì˜ë“±í¬ë¡œ 345",
            shelter_type="ë„ì„œê´€",
            capacity=300,
            distance_km=0.55,
            walking_minutes=7,
            latitude=37.5240,
            longitude=126.8920
        )
    ]
    
    user_profile = {
        "age_group": "ì„±ì¸",
        "mobility": "ì •ìƒ"
    }
    
    try:
        action_card, method = await llm_service.generate_action_card(
            disaster_type="í˜¸ìš°",
            location="ì„œìš¸ì‹œ ì˜ë“±í¬êµ¬",
            user_profile=user_profile,
            shelters=shelters
        )
        
        print(f"\nâœ… í–‰ë™ì¹´ë“œ ìƒì„± ì„±ê³µ (ë°©ë²•: {method})")
        print("-" * 60)
        print(action_card)
        print("-" * 60)
        
        # ëŒ€í”¼ì†Œ ì •ë³´ ì¶œë ¥
        print("\nğŸ“ ì¶”ì²œ ëŒ€í”¼ì†Œ:")
        for i, shelter in enumerate(shelters, 1):
            print(f"  {i}. {shelter.name} - ë„ë³´ {shelter.walking_minutes}ë¶„ ({shelter.distance_km}km)")
    
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬: {str(e)}")


async def test_multiple_scenarios():
    """ì—¬ëŸ¬ ì¬ë‚œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ë‹¤ì–‘í•œ ì¬ë‚œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    llm_service = LLMService()
    
    scenarios = [
        {"disaster_type": "ì§€ì§„", "location": "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬"},
        {"disaster_type": "íƒœí’", "location": "ë¶€ì‚°ì‹œ í•´ìš´ëŒ€êµ¬"},
        {"disaster_type": "í™”ì¬", "location": "ì„œìš¸ì‹œ ì˜ë“±í¬êµ¬"},
    ]
    
    shelter = ShelterInfo(
        id=uuid.uuid4(),
        name="ëŒ€í”¼ì†Œ",
        address="ì£¼ì†Œ",
        shelter_type="ì´ˆë“±í•™êµ",
        capacity=500,
        distance_km=0.3,
        walking_minutes=4,
        latitude=37.5263,
        longitude=126.8962
    )
    
    user_profile = {"age_group": "ì„±ì¸", "mobility": "ì •ìƒ"}
    
    for scenario in scenarios:
        print(f"\nğŸ“¢ {scenario['disaster_type']} ì‹œë‚˜ë¦¬ì˜¤ ({scenario['location']})")
        print("-" * 60)
        
        try:
            action_card, method = await llm_service.generate_action_card(
                disaster_type=scenario["disaster_type"],
                location=scenario["location"],
                user_profile=user_profile,
                shelters=[shelter]
            )
            
            print(f"ìƒì„± ë°©ë²•: {method}")
            print(action_card)
            print()
        
        except Exception as e:
            print(f"âŒ ì—ëŸ¬: {str(e)}\n")


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\nğŸš€ Qwen3 8B LLM í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # 1. Ollama API ì§ì ‘ í…ŒìŠ¤íŠ¸
    await test_qwen_direct()
    
    # 2. LLMService í…ŒìŠ¤íŠ¸
    await test_llm_service()
    
    # 3. ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    await test_multiple_scenarios()
    
    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())

